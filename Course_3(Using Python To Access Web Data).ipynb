{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Numbers in a Haystack\n",
    "\n",
    "# In this assignment you will read through and parse a file with text and numbers. You will extract all the numbers in the\n",
    "#file and compute the sum of the numbers.\n",
    "# Actual data: http://py4e-data.dr-chuck.net/regex_sum_1102535.txt \n",
    "# These links open in a new window. Make sure to save the file into the same folder as you will be writing your Python program.\n",
    "\n",
    "import re\n",
    "fname=open(\"sum.txt\")\n",
    "count=0\n",
    "sum=0\n",
    "for line in fname:\n",
    "    num=re.findall(\"[0-9]+\",line)\n",
    "    for i in num:\n",
    "        sum=sum+int(i)\n",
    "        count=count+1\n",
    "print(\"Total number of values:\",count,\"\\nsum:\",sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the HyperText Transport Protocol\n",
    "\n",
    "# You are to retrieve the following document using the HTTP protocol in a way that you can examine the HTTP Response headers.\n",
    "\n",
    "# http://data.pr4e.org/intro-short.txt\n",
    "# There are three ways that you might retrieve this web page and look at the response headers:\n",
    "\n",
    "# Preferred: Modify the socket1.py program to retrieve the above URL and print out the headers and data. Make sure to change the code to retrieve the above URL - the values are different for each URL.\n",
    "# Open the URL in a web browser with a developer console or FireBug and manually examine the headers that are returned.\n",
    "# Enter the header values in each of the fields below and press \"Submit\".\n",
    "\n",
    "# Last-Modified:\n",
    "\n",
    "# ETag:\n",
    "\n",
    "# Content-Length:\n",
    "\n",
    "# Cache-Control:\n",
    "\n",
    "# Content-Type:\n",
    "\n",
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Numbers from HTML using BeautifulSoup .The program will use urllib to read the HTML from the data files below,\n",
    "# and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "# Data: http://py4e-data.dr-chuck.net/comments_1102537.html \n",
    "# Data Format\n",
    "# The file is a table of names and comment counts. You can ignore most of the data in the\n",
    "# file except for lines like the following:\n",
    "\n",
    "# <tr><td>Modu</td><td><span class=\"comments\">90</span></td></tr>\n",
    "# <tr><td>Kenzie</td><td><span class=\"comments\">88</span></td></tr>\n",
    "# <tr><td>Hubert</td><td><span class=\"comments\">87</span></td></tr>\n",
    "# You are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers.\n",
    "import bs4\n",
    "from urllib.request import urlopen as ureq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "my_url=input(\"Enter Url: \")\n",
    "uclient=ureq(my_url)\n",
    "page_html=uclient.read()\n",
    "page_soup=soup(page_html,\"html.parser\")\n",
    "#print(page_soup)\n",
    "sum=0\n",
    "count=0\n",
    "tags=page_soup(\"span\")\n",
    "for tag in tags:\n",
    "    count=count+1\n",
    "    num=tag.contents[0]\n",
    "    sum=sum+int(num)\n",
    "print(\"Count: \",count)    \n",
    "print(\"sum: \",sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following Links in Python\n",
    "\n",
    "# In this assignment you will write a Python program,the program will use urllib to read the HTML from the data files below, \n",
    "# extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name\n",
    "# in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "# Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Rosalie.html\n",
    "# Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "# Hint: The first character of the name of the last page that you will load is: U\n",
    "# Strategy\n",
    "# The web pages tweak the height between the links and hide the page after a few seconds to make it difficult \n",
    "# for you to do the assignment without writing a Python program. But frankly with a little effort and patience\n",
    "# you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program.\n",
    "# But that is not the point. The point is to write a clever Python program to solve the program.\n",
    "import bs4\n",
    "from urllib.request import urlopen as ureq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "my_url=input(\"Enter Url: \")\n",
    "count=int(input(\"Enter count: \"))\n",
    "position=int(input(\"Enter position: \"))\n",
    "uclient=ureq(my_url)\n",
    "page_html=uclient.read()\n",
    "page_soup=soup(page_html,\"html.parser\")\n",
    "#print(page_soup)\n",
    "lst=[]\n",
    "tags=page_soup(\"a\")\n",
    "print(\"Retrieving: \",my_url)\n",
    "for tag in range(count):\n",
    "    #links=tag.get(\"href\",None)\n",
    "    link=tags[position-1].get(\"href\",None)\n",
    "    lst.append(link)\n",
    "\n",
    "    print(\"Retrieving: \",link)\n",
    "    uclient=ureq(link)\n",
    "    page_html=uclient.read()\n",
    "    page_soup=soup(page_html,\"html.parser\")\n",
    "    tags=page_soup(\"a\")\n",
    "    link=tags[position-1].get(\"href\",None)\n",
    "print(\"retrievedlast name is: \",lst[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Data from XML\n",
    "\n",
    "# In this assignment you will write a Python program ,The program will prompt for a URL, read the XML data from that URL\n",
    "# using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file.\n",
    "\n",
    "# Actual data: http://py4e-data.dr-chuck.net/comments_1102539.xml (Sum ends with 13)\n",
    "# Data Format and Approach\n",
    "# The data consists of a number of names and comment counts in XML as follows:\n",
    "\n",
    "# <comment>\n",
    "#   <name>Matthias</name>\n",
    "#   <count>97</count>\n",
    "# </comment>\n",
    "# You are to look through all the <comment> tags and find the <count> values sum the numbers. The closest sample code that shows\n",
    "# how to parse XML is geoxml.py.\n",
    "# To make the code a little simpler, you can use an XPath selector string to look through the entire tree of XML for any tag\n",
    "# named 'count' with the following line of code:\n",
    "# counts = tree.findall('.//count')\n",
    "# Take a look at the Python ElementTree documentation and look for the supported XPath syntax for details. \n",
    "# You could also work from the top of the XML down to the comments node and then loop through the child nodes of the\n",
    "# comments node.\n",
    "import urllib.request,urllib.parse,urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "link=input(\"Enter location:\")\n",
    "html=urllib.request.urlopen(link).read().decode()\n",
    "print(\"retrieving\",link)\n",
    "print(\"Received\",len(html),\"characters\")\n",
    "data=ET.fromstring(html)\n",
    "lst=data.findall(\"comments/comment\")\n",
    "#print(lst)\n",
    "sum=0\n",
    "count=0\n",
    "for item in lst:\n",
    "    count=count+1\n",
    "    items=item.find(\"count\").text\n",
    "    sum=sum+int(items)\n",
    "print(\"sum:\",sum)\n",
    "print(\"count:\",count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Data from JSON\n",
    "\n",
    "# In this assignment you will write a Python program ,The program will prompt for a URL, read the JSON data from that URL using\n",
    "# urllib and then parse and extract the comment counts from the JSON data, compute the sum of the numbers in the file and enter\n",
    "# the sum below:\n",
    "\n",
    "# Actual data: http://py4e-data.dr-chuck.net/comments_1102540.json (Sum ends with 55)\n",
    "# Data Format\n",
    "# The data consists of a number of names and comment counts in JSON as follows:\n",
    "\n",
    "# {\n",
    "#   comments: [\n",
    "#     {\n",
    "#       name: \"Matthias\"\n",
    "#       count: 97\n",
    "#     },\n",
    "#     {\n",
    "#       name: \"Geomer\"\n",
    "#       count: 97\n",
    "#     }\n",
    "#     ...\n",
    "#   ]\n",
    "# }\n",
    "import json\n",
    "import urllib.request,urllib.parse,urllib.error\n",
    "my_url=input(\"Enter location:\")\n",
    "json_file=urllib.request.urlopen(my_url).read().decode()\n",
    "print(\"Retreving\",my_url)\n",
    "info=json.loads(json_file)\n",
    "comments=info[\"comments\"]\n",
    "sum=0\n",
    "count=0\n",
    "for i in comments:\n",
    "    counts=i[\"count\"]\n",
    "    sum=sum+counts\n",
    "    count=count+1\n",
    "print(\"sum=\",sum)\n",
    "print(\"count=\",count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling a JSON API\n",
    "\n",
    "In this assignment you will write a Python program,The program will prompt for a location, contact a web service and \n",
    "retrieve JSON for the web service and parse that data, and retrieve the first place_id from the JSON. \n",
    "A place ID is a textual identifier that uniquely identifies a place as within Google Maps.\n",
    "API End Points\n",
    "\n",
    "To complete this assignment, you should use this API endpoint that has a static subset of the Google Data:\n",
    "\n",
    "http://py4e-data.dr-chuck.net/json?\n",
    "This API uses the same parameter (address) as the Google API. This API also has no rate limit so you can test as often as you like. If you visit the URL with no parameters, you get \"No address...\" response.\n",
    "To call the API, you need to include a key= parameter and provide the address that you are requesting as the address= parameter that is properly URL encoded using the urllib.parse.urlencode() function as shown in http://www.py4e.com/code3/geojson.py\n",
    "\n",
    "Make sure to check that your code is using the API endpoint is as shown above. You will get different results from the geojson and json endpoints so make sure you are using the same end point as this autograder is using.\n",
    "\n",
    "Test Data / Sample Execution\n",
    "\n",
    "You can test to see if your program is working with a location of \"South Federal University\" which will have a place_id of \"ChIJ0V94rPl_bIcR6KyIGL16ZQA\".\n",
    "\n",
    "$ python3 solution.py\n",
    "Enter location: South Federal University\n",
    "Retrieving http://...\n",
    "Retrieved 2458 characters\n",
    "Place id ChIJ0V94rPl_bIcR6KyIGL16ZQA\n",
    "Turn In\n",
    "\n",
    "Please run your program to find the place_id for this location:\n",
    "\n",
    "Saint Petersburg State University\n",
    "Make sure to enter the name and case exactly as above and enter the place_id and your Python code below. Hint: The first seven characters of the place_id are \"ChIJ6aX ...\"\n",
    "Make sure to retreive the data from the URL specified above and not the normal Google API. Your program should work with the Google API - but the place_id may not match for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    #print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    #print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    place_id = js['results'][0]['place_id']\n",
    "    print(\"place_id: \",place_id)\n",
    "    #print(\"location: \",location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
